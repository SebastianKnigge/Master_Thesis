\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Manning1999}
\citation{Winter2017}
\citation{DeJong1979}
\citation{Manning1999}
\citation{Noble1988}
\citation{Martinez2010}
\citation{Harris1951}
\citation{Blei2012}
\citation{Grossmann2004}
\citation{Jacobs1993}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Motivation and Organization of the Thesis}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Natural Language Processing and Information Retrival}{2}{section.2}}
\newlabel{sec:NLP}{{2}{2}{Natural Language Processing and Information Retrival}{section.2}{}}
\citation{Gelman2014}
\citation{Blei2003}
\@writefile{toc}{\contentsline {section}{\numberline {3}Discussed Models}{4}{section.3}}
\newlabel{sec:Models}{{3}{4}{Discussed Models}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}LDA Model}{4}{subsection.3.1}}
\newlabel{sec:LDA}{{3.1}{4}{LDA Model}{subsection.3.1}{}}
\citation{Hornik2011}
\citation{Blei2003}
\citation{Blei2012}
\citation{Powieser2012}
\citation{Dempster1977}
\citation{Wainwright2008}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The well-established plate diagram for the standard LDA model extended by the parameter $\delta $. The slightly bigger box represents the generative model of the corporis $M$ documents. The smaller plate represents the iterative generation process of the $N$ words of each document with the aid of the topics. See also "smoothed LDA model" in \cite  {Blei2003} for comparisons.\relax }}{5}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:PlateDiagram}{{1}{5}{The well-established plate diagram for the standard LDA model extended by the parameter $\delta $. The slightly bigger box represents the generative model of the corporis $M$ documents. The smaller plate represents the iterative generation process of the $N$ words of each document with the aid of the topics. See also "smoothed LDA model" in \cite {Blei2003} for comparisons.\relax }{figure.caption.1}{}}
\newlabel{posterior}{{1}{5}{LDA Model}{equation.3.1}{}}
\newlabel{joint}{{2}{5}{LDA Model}{equation.3.2}{}}
\newlabel{marginal}{{3}{5}{LDA Model}{equation.3.3}{}}
\citation{Jordan1999}
\citation{Wainwright2008}
\citation{Blei2003}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Variational EM Algorithm}{6}{subsubsection.3.1.1}}
\newlabel{marginal2}{{4}{6}{Variational EM Algorithm}{equation.3.4}{}}
\newlabel{KL}{{7}{6}{Variational EM Algorithm}{equation.3.7}{}}
\newlabel{equality}{{9}{6}{Variational EM Algorithm}{equation.3.9}{}}
\citation{Griffiths2006}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Gibbs Sampling}{7}{subsubsection.3.1.2}}
\newlabel{Gibbs}{{12}{7}{Gibbs Sampling}{equation.3.12}{}}
\citation{Griffiths2006}
\citation{Silge2017}
\citation{McCulloch1943}
\citation{Rb1958}
\newlabel{Gibbs:beta}{{13}{8}{Gibbs Sampling}{equation.3.13}{}}
\newlabel{Gibbs:theta}{{14}{8}{Gibbs Sampling}{equation.3.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Implementation}{8}{subsubsection.3.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Artifical Neural Networks}{8}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Development}{8}{subsubsection.3.2.1}}
\newlabel{ANN_development}{{3.2.1}{8}{Development}{subsubsection.3.2.1}{}}
\citation{Rb1958}
\citation{Mekherjee2019}
\citation{Bahjat2006}
\citation{Minsky1969}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic diagram of a simple perceptron by \cite  {Rb1958}\relax }}{10}{figure.caption.2}}
\newlabel{simple_perceptron}{{2}{10}{Schematic diagram of a simple perceptron by \cite {Rb1958}\relax }{figure.caption.2}{}}
\citation{Werbos1974}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Schematic diagram of an adapted Rosenblatt-perceptron network\relax }}{11}{figure.caption.3}}
\newlabel{adaption_perceptron}{{3}{11}{Schematic diagram of an adapted Rosenblatt-perceptron network\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Schematic diagram of an multi layer perceptron (MLP)\relax }}{11}{figure.caption.4}}
\newlabel{figure:MLP}{{4}{11}{Schematic diagram of an multi layer perceptron (MLP)\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Backpropagation}{11}{subsubsection.3.2.2}}
\citation{Chollet2018}
\citation{Hecht-Nielsen1988}
\citation{Chollet2018}
\citation{ProjectGutenberg}
\citation{gutenbergr}
\newlabel{network_chain}{{15}{12}{Backpropagation}{equation.3.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Implementation}{12}{subsubsection.3.2.3}}
\citation{tidytext}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis of Gutenberg Data}{13}{section.4}}
\newlabel{sec:analysis}{{4}{13}{Analysis of Gutenberg Data}{section.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example book corpus\relax }}{13}{table.caption.5}}
\newlabel{titles:5books}{{1}{13}{Example book corpus\relax }{table.caption.5}{}}
\citation{Silge2017}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Example of stop words from tidytext package\relax }}{14}{table.caption.6}}
\newlabel{stopwords}{{2}{14}{Example of stop words from tidytext package\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}LDA applied to Textbook Chapters}{15}{subsection.4.1}}
\newlabel{Example1}{{4.1}{15}{LDA applied to Textbook Chapters}{subsection.4.1}{}}
\citation{Bishop2006}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Boxplot of the all examples analyzed. Split for the number of categories (books) as well as for the calculation procedures (VEM algorithm and Gibbs sampling)\relax }}{17}{figure.caption.7}}
\newlabel{fig:comparison_boxplot}{{5}{17}{Boxplot of the all examples analyzed. Split for the number of categories (books) as well as for the calculation procedures (VEM algorithm and Gibbs sampling)\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}ANN applied to Textbook Chapters}{18}{subsection.4.2}}
\newlabel{sec:ANN.example}{{4.2}{18}{ANN applied to Textbook Chapters}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The network contains 3 layers. The input data is the the bag of words, which has dimension $V$. The first two layers are hidden, containing 64 and 46 neurons respectively. The last layer leads to the $k$ classes (here the number of topics or books). The activation function is softmax.\relax }}{18}{figure.caption.8}}
\newlabel{fig:network_structure}{{6}{18}{The network contains 3 layers. The input data is the the bag of words, which has dimension $V$. The first two layers are hidden, containing 64 and 46 neurons respectively. The last layer leads to the $k$ classes (here the number of topics or books). The activation function is softmax.\relax }{figure.caption.8}{}}
\citation{ESSVision2020}
\citation{Silge2017}
\@writefile{toc}{\contentsline {section}{\numberline {5}Analysis of EUROSTAT Documents}{20}{section.5}}
\newlabel{sec:example2}{{5}{20}{Analysis of EUROSTAT Documents}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Entire list of documents\relax }}{21}{table.caption.9}}
\newlabel{document_list}{{3}{21}{Entire list of documents\relax }{table.caption.9}{}}
\citation{Winter2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}LDA applied to EUROSTAT Documents}{22}{subsection.5.1}}
\newlabel{Example2}{{5.1}{22}{LDA applied to EUROSTAT Documents}{subsection.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Wordclouds for the clustered topics via LDA \IeC {\textendash } using tfidf word proportions\relax }}{23}{figure.caption.10}}
\newlabel{Wordclouds_tfidf}{{7}{23}{Wordclouds for the clustered topics via LDA â€“ using tfidf word proportions\relax }{figure.caption.10}{}}
\bibstyle{apalike}
\bibdata{MagLibrary}
\bibcite{Gelman2014}{A.~Gelman, 2014}
\bibcite{Dempster1977}{AP~Dempster, 1977}
\bibcite{Bahjat2006}{Bahjat, 2006}
\bibcite{Bishop2006}{Bishop, 2006}
\bibcite{Blei2012}{Blei, 2012}
\bibcite{Blei2003}{Blei, 2003}
\bibcite{Chollet2018}{Chollet, 2018}
\bibcite{DeJong1979}{DeJong, 1979}
\bibcite{ESSVision2020}{Eurostat, 2019}
\bibcite{Grossmann2004}{Grossmann, 2004}
\bibcite{Harris1951}{Harris, 1951}
\bibcite{ProjectGutenberg}{Hart, }
\bibcite{Hecht-Nielsen1988}{Hecht-Nielsen, 1988}
\bibcite{Hornik2011}{Hornik, 2011}
\bibcite{Jacobs1993}{Jacobs, 1993}
\bibcite{Jordan1999}{Jordan, 1999}
\bibcite{Griffiths2006}{M.~Steyvers, 2006}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{26}{appendix.A}}
\bibcite{Manning1999}{Manning, 1999}
\bibcite{Martinez2010}{Martinez, 2010}
\bibcite{McCulloch1943}{McCulloch and Pitts, 1943}
\bibcite{Minsky1969}{Minsky and Papert, 1969}
\bibcite{Mekherjee2019}{Mukherjee, 2019}
\bibcite{Noble1988}{Noble, 1988}
\bibcite{Powieser2012}{Powieser, 2012}
\bibcite{gutenbergr}{Robinson, 2018}
\bibcite{Rb1958}{Rosenblatt, 1958}
\bibcite{Silge2017}{Silge and Robinson, 2017}
\bibcite{tidytext}{Silge, 2019}
\bibcite{Wainwright2008}{Wainwright and Jordan, 2008}
\bibcite{Werbos1974}{Werbos, 1974}
\bibcite{Winter2017}{Winter, 2017}
\citation{Blei2003}
\citation{Rb1958}
