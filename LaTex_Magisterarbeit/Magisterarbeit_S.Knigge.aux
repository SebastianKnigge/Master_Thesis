\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\AC@reset@newl@bel
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\newacro{ANN}[\AC@hyperlink{ANN}{ANN}]{Artificial Neural Net}
\newacro{API}[\AC@hyperlink{API}{API}]{Application-Programming-Interface}
\newacro{CPU}[\AC@hyperlink{CPU}{CPU}]{Central Processing Unit }
\newacro{DNA}[\AC@hyperlink{DNA}{DNA}]{Deoxyribonucleic Acid}
\newacro{EM algorithm}[\AC@hyperlink{EM algorithm}{EM algorithm}]{Expectation\IeC {\textendash }Maximization algorithm}
\newacro{ESS}[\AC@hyperlink{ESS}{ESS}]{European Statistical System}
\newacro{EUROSTAT}[\AC@hyperlink{EUROSTAT}{EUROSTAT}]{European Statistical Office}
\newacro{GPU}[\AC@hyperlink{GPU}{GPU}]{Graphics Processing Unit}
\newacro{IR}[\AC@hyperlink{IR}{IR}]{Information Retrieval}
\newacro{KL divergence}[\AC@hyperlink{KL divergence}{KL divergence}]{Kullback\IeC {\textendash }Leibler divergence}
\newacro{LDA}[\AC@hyperlink{LDA}{LDA}]{Latent Dirichlet Allocation}
\newacro{M-P neuron}[\AC@hyperlink{M-P neuron}{M-P neuron}]{McCulloch-Pitts neuron}
\newacro{NLP}[\AC@hyperlink{NLP}{NLP}]{Natural Language Processing}
\newacro{tf}[\AC@hyperlink{tf}{tf}]{term frequency}
\newacro{tf-idf}[\AC@hyperlink{tf-idf}{tf-idf}]{term frequency inverse document frequency}
\newacro{VEM algorithm}[\AC@hyperlink{VEM algorithm}{VEM algorithm}]{Variational Expectation\IeC {\textendash }Maximization algorithm}
\citation{Manning1999}
\citation{Winter2017}
\citation{DeJong1979}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction and Organization of the Thesis}{6}{section.1}}
\citation{Manning1999}
\citation{Noble1988}
\citation{Martinez2010}
\citation{Harris1951}
\citation{Blei2012}
\citation{Grossmann2004}
\citation{Jacobs1993}
\@writefile{toc}{\contentsline {section}{\numberline {2}Natural Language Processing and Information Retrieval}{7}{section.2}}
\newlabel{sec:NLP}{{2}{7}{Natural Language Processing and Information Retrieval}{section.2}{}}
\citation{Gelman2014}
\citation{Blei2003}
\@writefile{toc}{\contentsline {section}{\numberline {3}Discussed Models}{9}{section.3}}
\newlabel{sec:Models}{{3}{9}{Discussed Models}{section.3}{}}
\citation{Hornik2011}
\citation{Blei2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}LDA Model}{10}{subsection.3.1}}
\newlabel{sec:LDA}{{3.1}{10}{LDA Model}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Well-established plate diagram for the standard LDA model extended by the parameter $\delta $. The slightly bigger box represents the generative model of the corporis $M$ documents. The smaller plate represents the iterative generation process of the $N$ words of each document with the aid of the topics. See also ``smoothed LDA model" in \cite  {Blei2003} for comparisons.\relax }}{10}{figure.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:PlateDiagram}{{1}{10}{Well-established plate diagram for the standard LDA model extended by the parameter $\delta $. The slightly bigger box represents the generative model of the corporis $M$ documents. The smaller plate represents the iterative generation process of the $N$ words of each document with the aid of the topics. See also ``smoothed LDA model" in \cite {Blei2003} for comparisons.\relax }{figure.caption.4}{}}
\citation{Blei2012}
\citation{Powieser2012}
\citation{Dempster1977}
\citation{Wainwright2008}
\newlabel{posterior}{{1}{11}{LDA Model}{equation.3.1}{}}
\newlabel{joint}{{2}{11}{LDA Model}{equation.3.2}{}}
\newlabel{marginal}{{3}{11}{LDA Model}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Variational EM Algorithm}{11}{subsubsection.3.1.1}}
\citation{Jordan1999}
\citation{Wainwright2008}
\citation{Blei2003}
\citation{Griffiths2006}
\newlabel{marginal2}{{4}{12}{Variational EM Algorithm}{equation.3.4}{}}
\newlabel{KL}{{7}{12}{Variational EM Algorithm}{equation.3.7}{}}
\newlabel{equality}{{9}{12}{Variational EM Algorithm}{equation.3.9}{}}
\citation{Griffiths2006}
\citation{Silge2017}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Gibbs Sampling}{13}{subsubsection.3.1.2}}
\newlabel{Gibbs}{{12}{13}{Gibbs Sampling}{equation.3.12}{}}
\newlabel{Gibbs:beta}{{13}{13}{Gibbs Sampling}{equation.3.13}{}}
\newlabel{Gibbs:theta}{{14}{13}{Gibbs Sampling}{equation.3.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Implementation}{13}{subsubsection.3.1.3}}
\citation{McCulloch1943}
\citation{Rb1958}
\citation{Rb1958}
\citation{Mekherjee2019}
\citation{Bahjat2006}
\citation{Minsky1969}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Artifical Neural Networks}{14}{subsection.3.2}}
\newlabel{ANN_chapter}{{3.2}{14}{Artifical Neural Networks}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Development}{14}{subsubsection.3.2.1}}
\newlabel{ANN_development}{{3.2.1}{14}{Development}{subsubsection.3.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic diagram of a simple perceptron by \cite  {Rb1958}\relax }}{15}{figure.caption.5}}
\newlabel{simple_perceptron}{{2}{15}{Schematic diagram of a simple perceptron by \cite {Rb1958}\relax }{figure.caption.5}{}}
\citation{Werbos1974}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Schematic diagram of an adapted Rosenblatt-perceptron network\relax }}{16}{figure.caption.6}}
\newlabel{adaption_perceptron}{{3}{16}{Schematic diagram of an adapted Rosenblatt-perceptron network\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Backpropagation}{16}{subsubsection.3.2.2}}
\citation{Chollet2018}
\citation{Hecht-Nielsen1988}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Schematic diagram of an multi layer perceptron (MLP)\relax }}{17}{figure.caption.7}}
\newlabel{figure:MLP}{{4}{17}{Schematic diagram of an multi layer perceptron (MLP)\relax }{figure.caption.7}{}}
\newlabel{network_chain}{{15}{17}{Backpropagation}{equation.3.15}{}}
\citation{Chollet2018}
\citation{ProjectGutenberg}
\citation{gutenbergr}
\citation{tidytext}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Implementation}{18}{subsubsection.3.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis of Gutenberg Data}{18}{section.4}}
\newlabel{sec:analysis}{{4}{18}{Analysis of Gutenberg Data}{section.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example book corpus\relax }}{18}{table.caption.8}}
\newlabel{titles:5books}{{1}{18}{Example book corpus\relax }{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Example of stop words from tidytext package\relax }}{19}{table.caption.9}}
\newlabel{stopwords}{{2}{19}{Example of stop words from tidytext package\relax }{table.caption.9}{}}
\citation{Silge2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}LDA applied to Textbook Chapters}{20}{subsection.4.1}}
\newlabel{Example1}{{4.1}{20}{LDA applied to Textbook Chapters}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Fitting time of Example 1 (6 Books) depending on the reduction of the bag-of-words. A big number of fraction stands for a bigger bag-of-words, i.e. less reduction. The blue smoothed curve is a Loess kernel. The drop in fitting time for the last few values of fraction might result from parallelization reasons.\relax }}{22}{figure.caption.10}}
\newlabel{fitting_time_tfidf}{{5}{22}{Fitting time of Example 1 (6 Books) depending on the reduction of the bag-of-words. A big number of fraction stands for a bigger bag-of-words, i.e. less reduction. The blue smoothed curve is a Loess kernel. The drop in fitting time for the last few values of fraction might result from parallelization reasons.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Misclassification rate depending on the reduction of the bag-of-words. Examples 1 and 2 with 6 books each are considered here. A big number of fraction stands for a bigger bag-of-words, i.e. less reduction. The colored smoothed curves are a Loess kernels.\relax }}{24}{figure.caption.11}}
\newlabel{misc.rate_tfidf}{{6}{24}{Misclassification rate depending on the reduction of the bag-of-words. Examples 1 and 2 with 6 books each are considered here. A big number of fraction stands for a bigger bag-of-words, i.e. less reduction. The colored smoothed curves are a Loess kernels.\relax }{figure.caption.11}{}}
\citation{Bishop2006}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Boxplot of the all examples analyzed. Split for the number of categories (books) as well as for the calculation procedures (VEM algorithm and Gibbs sampling).\relax }}{25}{figure.caption.12}}
\newlabel{fig:comparison_boxplot}{{7}{25}{Boxplot of the all examples analyzed. Split for the number of categories (books) as well as for the calculation procedures (VEM algorithm and Gibbs sampling).\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}ANN applied to Textbook Chapters}{25}{subsection.4.2}}
\newlabel{sec:ANN.example}{{4.2}{25}{ANN applied to Textbook Chapters}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Misclassification rate depending on the reduction of the bag-of-words. Example 3 using 10 books is considered here. A big number of fraction stands for a bigger bag-of-words, i.e. less reduction. The violet smoothed curve is a Loess kernel. Nothe, that the local minumum in the misclasfication rate will be referred to as ``low-dimension embedding anomaly"\relax }}{26}{figure.caption.13}}
\newlabel{misc.rate_10_tfidf}{{8}{26}{Misclassification rate depending on the reduction of the bag-of-words. Example 3 using 10 books is considered here. A big number of fraction stands for a bigger bag-of-words, i.e. less reduction. The violet smoothed curve is a Loess kernel. Nothe, that the local minumum in the misclasfication rate will be referred to as ``low-dimension embedding anomaly"\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The network contains 3 layers. The input data is the the bag-of-words, which has dimension $V$. The first two layers are hidden, containing 64 and 46 neurons respectively. The last layer leads to the $k$ classes (here the number of topics or books). The activation function is softmax.\relax }}{27}{figure.caption.14}}
\newlabel{fig:network_structure}{{9}{27}{The network contains 3 layers. The input data is the the bag-of-words, which has dimension $V$. The first two layers are hidden, containing 64 and 46 neurons respectively. The last layer leads to the $k$ classes (here the number of topics or books). The activation function is softmax.\relax }{figure.caption.14}{}}
\citation{ESSVision2020}
\citation{Silge2017}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Example 1 (6 books): Performance for different embeddings\relax }}{28}{table.caption.15}}
\newlabel{performancematrix_Ex1}{{3}{28}{Example 1 (6 books): Performance for different embeddings\relax }{table.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Example 2 (6 books): Performance for different embeddings\relax }}{28}{table.caption.16}}
\newlabel{performancematrix_Ex2}{{4}{28}{Example 2 (6 books): Performance for different embeddings\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Analysis of EUROSTAT Documents}{28}{section.5}}
\newlabel{sec:example2}{{5}{28}{Analysis of EUROSTAT Documents}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Entire list of documents\relax }}{29}{table.caption.17}}
\newlabel{document_list}{{5}{29}{Entire list of documents\relax }{table.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}LDA applied to EUROSTAT Documents}{30}{subsection.5.1}}
\newlabel{Example2}{{5.1}{30}{LDA applied to EUROSTAT Documents}{subsection.5.1}{}}
\citation{Winter2017}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Wordclouds for the clustered topics via LDA \IeC {\textendash } using tfidf word proportions\relax }}{32}{figure.caption.18}}
\newlabel{Wordclouds_tfidf}{{10}{32}{Wordclouds for the clustered topics via LDA â€“ using tfidf word proportions\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Fitting LDA via \textit  {tf-idf} embeding to EUROSTAT documents. Misclassification rates depending on the reduction of the bag-of-words. A big number of fraction stands for a larger bag-of-words, i.e. less reduction. The green smoothed curve is a Loess kernel.\relax }}{33}{figure.caption.19}}
\newlabel{misc.ratio_tfidf}{{11}{33}{Fitting LDA via \textit {tf-idf} embeding to EUROSTAT documents. Misclassification rates depending on the reduction of the bag-of-words. A big number of fraction stands for a larger bag-of-words, i.e. less reduction. The green smoothed curve is a Loess kernel.\relax }{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces LDA via Gibbs Sampling - different embedding methods\relax }}{34}{table.caption.20}}
\newlabel{comp0.85}{{6}{34}{LDA via Gibbs Sampling - different embedding methods\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}ANN applied to EUROSTAT Documents}{34}{subsection.5.2}}
\newlabel{Example2_ANN}{{5.2}{34}{ANN applied to EUROSTAT Documents}{subsection.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces ANN for EUROSTAT documents - different embedding methods\relax }}{35}{table.caption.21}}
\newlabel{ANN_results}{{7}{35}{ANN for EUROSTAT documents - different embedding methods\relax }{table.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Plot of the learning process for the used neural network to classify the EUROSTAT documents.\relax }}{36}{figure.caption.22}}
\newlabel{learning:plot}{{12}{36}{Plot of the learning process for the used neural network to classify the EUROSTAT documents.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{37}{section.6}}
\newlabel{sec:conclusions}{{6}{37}{Conclusions}{section.6}{}}
\citation{Winter2017}
\bibstyle{apalike}
\bibdata{MagLibrary}
\bibcite{Gelman2014}{A.~Gelman, 2014}
\bibcite{Dempster1977}{AP~Dempster, 1977}
\bibcite{Bahjat2006}{Bahjat, 2006}
\bibcite{Bishop2006}{Bishop, 2006}
\bibcite{Blei2012}{Blei, 2012}
\bibcite{Blei2003}{Blei, 2003}
\bibcite{Chollet2018}{Chollet, 2018}
\bibcite{DeJong1979}{DeJong, 1979}
\bibcite{ESSVision2020}{Eurostat, 2019}
\bibcite{Grossmann2004}{Grossmann, 2004}
\bibcite{Harris1951}{Harris, 1951}
\bibcite{ProjectGutenberg}{Hart, }
\bibcite{Hecht-Nielsen1988}{Hecht-Nielsen, 1988}
\bibcite{Hornik2011}{Hornik, 2011}
\bibcite{Jacobs1993}{Jacobs, 1993}
\bibcite{Jordan1999}{Jordan, 1999}
\bibcite{Griffiths2006}{M.~Steyvers, 2006}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{39}{appendix.A}}
\bibcite{Manning1999}{Manning, 1999}
\bibcite{Martinez2010}{Martinez, 2010}
\bibcite{McCulloch1943}{McCulloch and Pitts, 1943}
\bibcite{Minsky1969}{Minsky and Papert, 1969}
\bibcite{Mekherjee2019}{Mukherjee, 2019}
\bibcite{Noble1988}{Noble, 1988}
\bibcite{Powieser2012}{Powieser, 2012}
\bibcite{gutenbergr}{Robinson, 2018}
\bibcite{Rb1958}{Rosenblatt, 1958}
\bibcite{Silge2017}{Silge and Robinson, 2017}
\bibcite{tidytext}{Silge, 2019}
\bibcite{Wainwright2008}{Wainwright and Jordan, 2008}
\bibcite{Werbos1974}{Werbos, 1974}
\bibcite{Winter2017}{Winter, 2017}
\citation{Blei2003}
\citation{Rb1958}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Documentation of LDA - Gutenberg Data Examples}{42}{subsection.A.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Documentation of ANN - Gutenberg Data Examples}{67}{subsection.A.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Documentation of LDA - EUROSTAT Documents Example}{79}{subsection.A.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Documentation of ANN - EUROSTAT Documents Example}{101}{subsection.A.4}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Zusammenfassung}{110}{appendix.B}}
