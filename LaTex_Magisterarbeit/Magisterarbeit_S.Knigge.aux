\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Manning1999}
\citation{Noble1988}
\citation{Martinez2010}
\citation{Harris1951}
\citation{Blei2012}
\citation{Grossmann2004}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Natural Language Processing and Information Retrival}{1}{section.1}}
\citation{Gelman2014}
\citation{Blei2003}
\@writefile{toc}{\contentsline {section}{\numberline {2}Discussed models}{2}{section.2}}
\citation{Hornik2011}
\citation{Blei2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}LDA model}{3}{subsection.2.1}}
\newlabel{sec:LDA}{{2.1}{3}{LDA model}{subsection.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The well-established plate diagram for the standard LDA model extended by the parameter $\delta $. The slightly bigger box represents the generative model of the corporis $M$ documents. The smaller plate represents the iterative generation process of the $N$ words of each document with the aid of the topics. See also "smoothed LDA model" in \cite  {Blei2003} for comparisons.\relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:PlateDiagram}{{1}{3}{The well-established plate diagram for the standard LDA model extended by the parameter $\delta $. The slightly bigger box represents the generative model of the corporis $M$ documents. The smaller plate represents the iterative generation process of the $N$ words of each document with the aid of the topics. See also "smoothed LDA model" in \cite {Blei2003} for comparisons.\relax }{figure.caption.1}{}}
\citation{Blei2012}
\citation{Powieser2012}
\citation{Dempster1977}
\citation{Wainwright2008}
\newlabel{posterior}{{1}{4}{LDA model}{equation.2.1}{}}
\newlabel{joint}{{2}{4}{LDA model}{equation.2.2}{}}
\newlabel{marginal}{{3}{4}{LDA model}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Variational EM algorithm}{4}{subsubsection.2.1.1}}
\citation{Jordan1999}
\citation{Wainwright2008}
\citation{Blei2003}
\citation{Griffiths2006}
\newlabel{marginal2}{{4}{5}{Variational EM algorithm}{equation.2.4}{}}
\newlabel{KL}{{7}{5}{Variational EM algorithm}{equation.2.7}{}}
\newlabel{equality}{{9}{5}{Variational EM algorithm}{equation.2.9}{}}
\citation{Griffiths2006}
\citation{Silge2017}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Gibbs sampling}{6}{subsubsection.2.1.2}}
\newlabel{Gibbs}{{12}{6}{Gibbs sampling}{equation.2.12}{}}
\newlabel{Gibbs:beta}{{13}{6}{Gibbs sampling}{equation.2.13}{}}
\newlabel{Gibbs:theta}{{14}{6}{Gibbs sampling}{equation.2.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Implementation}{6}{subsubsection.2.1.3}}
\citation{McCulloch1943}
\citation{Rb1958}
\citation{Rb1958}
\citation{Mekherjee2019}
\citation{Bahjat2006}
\citation{Minsky1969}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Artifical Neural Networks}{7}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Development}{7}{subsubsection.2.2.1}}
\newlabel{ANN_development}{{2.2.1}{7}{Development}{subsubsection.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic diagram of a simple perceptron by \cite  {Rb1958}\relax }}{8}{figure.caption.2}}
\newlabel{simple_perceptron}{{2}{8}{Schematic diagram of a simple perceptron by \cite {Rb1958}\relax }{figure.caption.2}{}}
\citation{Werbos1974}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Schematic diagram of an adapted Rosenblatt-perceptron network\relax }}{9}{figure.caption.3}}
\newlabel{adaption_perceptron}{{3}{9}{Schematic diagram of an adapted Rosenblatt-perceptron network\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Backpropagation}{9}{subsubsection.2.2.2}}
\citation{Chollet2018}
\citation{Hecht-Nielsen1988}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Schematic diagram of an multi layer perceptron (MLP)\relax }}{10}{figure.caption.4}}
\newlabel{figure:MLP}{{4}{10}{Schematic diagram of an multi layer perceptron (MLP)\relax }{figure.caption.4}{}}
\newlabel{network_chain}{{15}{10}{Backpropagation}{equation.2.15}{}}
\citation{Chollet2018}
\citation{ProjectGutenberg}
\citation{gutenbergr}
\citation{tidytext}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Implementation}{11}{subsubsection.2.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analysis of textbook chapters}{11}{section.3}}
\newlabel{sec:analysis}{{3}{11}{Analysis of textbook chapters}{section.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example book corpus\relax }}{11}{table.caption.5}}
\newlabel{titles:5books}{{1}{11}{Example book corpus\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Example of stop words from tidytext package\relax }}{12}{table.caption.6}}
\newlabel{stopwords}{{2}{12}{Example of stop words from tidytext package\relax }{table.caption.6}{}}
\citation{Silge2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}LDA applied on Gutenberg data}{13}{subsection.3.1}}
\citation{Bishop2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}ANN applied to Gutenberg data}{15}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Boxplot of the all examples analyzed. Split for the number of categories (books) as well as for the calculation procedures (VEM algorithm and Gibbs sampling)\relax }}{16}{figure.caption.7}}
\newlabel{fig:comparison_boxplot}{{5}{16}{Boxplot of the all examples analyzed. Split for the number of categories (books) as well as for the calculation procedures (VEM algorithm and Gibbs sampling)\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The network contains 3 layers. The input data is the the bag of words, which has dimension $V$. The first two layers are hidden, containing 64 and 46 neurons respectively. The last layer leads to the $k$ classes (here the number of topics or books). The activation function is softmax.\relax }}{17}{figure.caption.8}}
\newlabel{fig:network_structure}{{6}{17}{The network contains 3 layers. The input data is the the bag of words, which has dimension $V$. The first two layers are hidden, containing 64 and 46 neurons respectively. The last layer leads to the $k$ classes (here the number of topics or books). The activation function is softmax.\relax }{figure.caption.8}{}}
\citation{ESSVision2020}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis of EUROSTAT Documents}{18}{section.4}}
\newlabel{sec:example2}{{4}{18}{Analysis of EUROSTAT Documents}{section.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Entire list of documents\relax }}{19}{table.caption.9}}
\newlabel{}{{3}{19}{Entire list of documents\relax }{table.caption.9}{}}
\bibstyle{apalike}
\bibdata{MagLibrary}
\bibcite{Gelman2014}{A.~Gelman, 2014}
\bibcite{Dempster1977}{AP~Dempster, 1977}
\bibcite{Bahjat2006}{Bahjat, 2006}
\bibcite{Bishop2006}{Bishop, 2006}
\bibcite{Blei2012}{Blei, 2012}
\bibcite{Blei2003}{Blei, 2003}
\bibcite{Chollet2018}{Chollet, 2018}
\bibcite{ESSVision2020}{Eurostat, 2019}
\bibcite{Grossmann2004}{Grossmann, 2004}
\bibcite{Harris1951}{Harris, 1951}
\bibcite{ProjectGutenberg}{Hart, }
\bibcite{Hecht-Nielsen1988}{Hecht-Nielsen, 1988}
\bibcite{Hornik2011}{Hornik, 2011}
\bibcite{Jordan1999}{Jordan, 1999}
\bibcite{Griffiths2006}{M.~Steyvers, 2006}
\bibcite{Manning1999}{Manning, 1999}
\bibcite{Martinez2010}{Martinez, 2010}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{21}{appendix.A}}
\bibcite{McCulloch1943}{McCulloch and Pitts, 1943}
\bibcite{Minsky1969}{Minsky and Papert, 1969}
\bibcite{Mekherjee2019}{Mukherjee, 2019}
\bibcite{Noble1988}{Noble, 1988}
\bibcite{Powieser2012}{Powieser, 2012}
\bibcite{gutenbergr}{Robinson, 2018}
\bibcite{Rb1958}{Rosenblatt, 1958}
\bibcite{Silge2017}{Silge and Robinson, 2017}
\bibcite{tidytext}{Silge, 2019}
\bibcite{Wainwright2008}{Wainwright and Jordan, 2008}
\bibcite{Werbos1974}{Werbos, 1974}
\citation{Blei2003}
\citation{Rb1958}
