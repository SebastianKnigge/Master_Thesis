\select@language {english}
\contentsline {figure}{\numberline {1}{\ignorespaces Well-established plate diagram for the standard LDA model extended by the parameter $\delta $. The slightly bigger box represents the generative model of the corporis $M$ documents. The smaller plate represents the iterative generation process of the $N$ words of each document with the aid of the topics. See also ``smoothed LDA model" in \cite {Blei2003} for comparisons.\relax }}{10}{figure.caption.4}
\contentsline {figure}{\numberline {2}{\ignorespaces Schematic diagram of a simple perceptron by \cite {Rb1958}\relax }}{15}{figure.caption.5}
\contentsline {figure}{\numberline {3}{\ignorespaces Schematic diagram of an adapted Rosenblatt-perceptron network\relax }}{16}{figure.caption.6}
\contentsline {figure}{\numberline {4}{\ignorespaces Schematic diagram of an multi layer perceptron (MLP)\relax }}{17}{figure.caption.7}
\contentsline {figure}{\numberline {5}{\ignorespaces Fitting time of Example 1 (6 Books) depending on the reduction of the bag-of-words. A big number of fraction stands for a bigger bag-of-words, i.e. less reduction. The blue smoothed curve is a Loess kernel. The drop in fitting time for the last few values of fraction might result from parallelization reasons.\relax }}{22}{figure.caption.10}
\contentsline {figure}{\numberline {6}{\ignorespaces Misclassification rate depending on the reduction of the bag-of-words. Examples 1 and 2 with 6 books each are considered here. A big number of fraction stands for a bigger bag-of-words, i.e. less reduction. The colored smoothed curves are a Loess kernels.\relax }}{24}{figure.caption.11}
\contentsline {figure}{\numberline {7}{\ignorespaces Boxplot of the all examples analyzed. Split for the number of categories (books) as well as for the calculation procedures (VEM algorithm and Gibbs sampling).\relax }}{25}{figure.caption.12}
\contentsline {figure}{\numberline {8}{\ignorespaces Misclassification rate depending on the reduction of the bag-of-words. Example 3 using 10 books is considered here. A big number of fraction stands for a bigger bag-of-words, i.e. less reduction. The violet smoothed curve is a Loess kernel. Nothe, that the local minumum in the misclasfication rate will be referred to as ``low-dimension embedding anomaly"\relax }}{26}{figure.caption.13}
\contentsline {figure}{\numberline {9}{\ignorespaces The network contains 3 layers. The input data is the the bag-of-words, which has dimension $V$. The first two layers are hidden, containing 64 and 46 neurons respectively. The last layer leads to the $k$ classes (here the number of topics or books). The activation function is softmax.\relax }}{27}{figure.caption.14}
\contentsline {figure}{\numberline {10}{\ignorespaces Wordclouds for the clustered topics via LDA \IeC {\textendash } using tfidf word proportions\relax }}{32}{figure.caption.18}
\contentsline {figure}{\numberline {11}{\ignorespaces Fitting LDA via \textit {tf-idf} embeding to EUROSTAT documents. Misclassification rates depending on the reduction of the bag-of-words. A big number of fraction stands for a larger bag-of-words, i.e. less reduction. The green smoothed curve is a Loess kernel.\relax }}{33}{figure.caption.19}
\contentsline {figure}{\numberline {12}{\ignorespaces Plot of the learning process for the used neural network to classify the EUROSTAT documents.\relax }}{36}{figure.caption.22}
