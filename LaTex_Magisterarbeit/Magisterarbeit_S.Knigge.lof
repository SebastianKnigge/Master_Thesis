\select@language {english}
\contentsline {figure}{\numberline {1}{\ignorespaces The well-established plate diagram for the standard LDA model extended by the parameter $\delta $. The slightly bigger box represents the generative model of the corporis $M$ documents. The smaller plate represents the iterative generation process of the $N$ words of each document with the aid of the topics. See also "smoothed LDA model" in \cite {Blei2003} for comparisons.\relax }}{4}{figure.caption.1}
\contentsline {figure}{\numberline {2}{\ignorespaces Schematic diagram of a simple perceptron by \cite {Rb1958}\relax }}{9}{figure.caption.2}
\contentsline {figure}{\numberline {3}{\ignorespaces Schematic diagram of an adapted Rosenblatt-perceptron network\relax }}{10}{figure.caption.3}
\contentsline {figure}{\numberline {4}{\ignorespaces Schematic diagram of an multi layer perceptron (MLP)\relax }}{10}{figure.caption.4}
\contentsline {figure}{\numberline {5}{\ignorespaces Boxplot of the all examples analyzed. Split for the number of categories (books) as well as for the calculation procedures (VEM algorithm and Gibbs sampling)\relax }}{16}{figure.caption.7}
\contentsline {figure}{\numberline {6}{\ignorespaces The network contains 3 layers. The input data is the the bag of words, which has dimension $V$. The first two layers are hidden, containing 64 and 46 neurons respectively. The last layer leads to the $k$ classes (here the number of topics or books). The activation function is softmax.\relax }}{17}{figure.caption.8}
\contentsline {figure}{\numberline {7}{\ignorespaces Wordclouds for the clustered topics via LDA \IeC {\textendash } using tfidf word proportions\relax }}{22}{figure.caption.10}
