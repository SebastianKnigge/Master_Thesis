---
title: "LDA Documentation"
author: "Sebastian Knigge"
date: "6 8 2019"
output: 
  pdf_document:
    number_sections: true
---

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stargazer)
```

Following packages were used in this script:

```{r, message=FALSE, warning=FALSE}
# loading packages
library(gutenbergr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(udpipe)
library(topicmodels)
library(ggplot2)
library(parallel)
library(foreach)
```

# Example 1 (6 Books)

## Get data

### Definition of functions

This is the essential step for setting up the LDA model. These functions include the sampling procedure from the *gutenbergr* library.

```{r, message=FALSE, warning=FALSE}
sampling_books <- function(seed=1234, n=20){
  # sample n books from the whole library
  set.seed(seed)
  gutenberg_works() %>% 
    # select works with title
    dplyr::filter(!is.na(title)&!is.na(gutenberg_bookshelf)) %>% 
    # set the sample sitze
    sample_n(n) %>%
    # set a special download link
    gutenberg_download(
      mirror = "http://mirrors.xmission.com/gutenberg/")
}

set_up_books <- function(n_books=4, seed=1992){
  # initial book sample
  books <- sampling_books(n=n_books, seed=seed)
  by_chapter <- books %>%
    group_by(gutenberg_id) %>%
    # split in chapters
    mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
    ungroup() %>%
    # exclude books without chapters
    dplyr::filter(chapter > 0) 
  return(by_chapter)
}

shorten_titles <- function(titles){
  # shorten very long book titles by setting 
  # a subset of characters of the first line
  # of the title
  sub_inds <- titles %>% 
    regexpr(pattern="\\n|\\r")-1
  sub_inds[sub_inds<0] <- nchar(titles)[sub_inds<0]
  sub_inds <- pmin(sub_inds, 45)
  titles %>% 
    substr(1,sub_inds)
}

get_titles <- function(x, n_books){
  # get the sampled gutenberg_ids
  unique_ids <- x %>% 
    select(gutenberg_id) %>% 
    unique() %>% unlist()
  # get the titles
  titles <- gutenberg_works() %>% 
    dplyr::filter(gutenberg_id %in% unique_ids) %>% 
    select(gutenberg_id, title, author) %>% 
    mutate(title=shorten_titles(title))
  # get the number of gutenberg ids
  len <- nrow(titles)
  if(n_books!=len) warning(paste("--- ",n_books-len, 
                                 " books have 0 chapters --- "))
  # the output as a list  
  ret <- list(
    titles=titles,
    len=len
  )
  return(ret)
}

append_by_chapter <- function(x=by_chapter, n_books, seed_index=1){
  # append the books matrix until
  # we get the desired number of books n_books
  titles <- get_titles(x, n_books)
  n <- titles$len
  while (n<n_books) {
    book2add <- sampling_books(n=1, seed=seed_index)
    by_chapter_add <- book2add %>%
      group_by(gutenberg_id) %>%
      # split in chapters
      mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
      ungroup() %>%
      # exclude books without chapters
      dplyr::filter(chapter > 2)
    titles2add <- get_titles(by_chapter_add, 1)
    # adding the book to by_chapter if there are chapters in the 
    # book plus it is not in the data already
    if (titles2add$len==1) if(!titles2add$titles$gutenberg_id%in%titles$titles$gutenberg_id) {
      x <- bind_rows(x, by_chapter_add)
    }
    n<-get_titles(x, n)$len
    seed_index <- seed_index+1
  }
  return(x)
}

exclude_stop_words <- function(x){
  # unite chapter and document title
  by_chapter_word <- x %>%
    unite(document, gutenberg_id, chapter) %>%
    # split into words
    unnest_tokens(word, text)
  # import tibble stop words
  data(stop_words)
  # find document-word counts
  word_counts <- by_chapter_word %>%
    # exclude stop words
    anti_join(stop_words) %>%
    # count each word by chapter
    count(document, word, sort = TRUE) %>%
    ungroup()
  return(word_counts)
}

convert_to_dtm <- function(x, minfq = 2){
  # get into a format lda can handle
  chapters_dtm <- x %>%
    select(doc_id=document, term=word, freq=n) %>%
    document_term_matrix() %>%
    # reduce by low frequencies
    dtm_remove_lowfreq(minfreq = minfq)
  return(chapters_dtm)
}


convert_to_dtm_2 <- function(x, n=n, minfq = 2, top=10000){
  # get into a format lda can handle
  chapters_dtm <- x %>%
    select(doc_id=document, term=word, freq=n) %>%
    document_term_matrix() %>%
    # reduce by low frequencies
    dtm_remove_tfidf(top=top)
  return(chapters_dtm)
}
```

Note:

  * good seperation for 4 topics:
    + seed=12345
    + seed=54321
  * for 6 books:
    + seed=222
    + seed 101
  * for 10 books:
    + seed=54321
    + seed=123456


### Sample corpus

Now we can use all these functions to get to the initial corpus sample. In this example $6$ books are choosen.

```{r setup_books, chache=TRUE, warning=TRUE}
n_books <- 6
by_chapter <- set_up_books(n_books=n_books, seed=222)
get_titles(by_chapter, n_books)
```

The function *set_up_books()* returns a warning that several books seem to consist of only one chapter. In order to get a copus consisting out of $6$ books, the function *append_by_chaper()* is used, which fills up the corpus to the desired number of books.

```{r append_by_chapter, chache=TRUE, warning=FALSE}
appended_by_chapter <- append_by_chapter(x=by_chapter, n_books = n_books)
word_counts <- exclude_stop_words(appended_by_chapter)
```

In table \ref{titles:6books} the sampled titles for the book sample with the seed $222$ are displayed. It appears through the function *append_by:chapter(9)* one book was added, called "Clotelle: A Tale of the Southern States".


```{r, results="asis"}
titles <- get_titles(appended_by_chapter, n_books)
titles$titles %>% stargazer(summary=FALSE, font.size = "footnotesize", 
                            header=FALSE, title="Book-titles", rownames=FALSE,
                            label="titles:6books")
```

We also want to check if the book categories are different. See Table (\ref{categories:6books}) for comparison.
```{r categories, results="asis"}
gbids <- titles$titles$gutenberg_id 
categories <- gutenberg_works() %>% 
  filter(gutenberg_id %in% gbids) %>% 
  select(gutenberg_id, gutenberg_bookshelf)
categories %>% stargazer(summary=FALSE, font.size = "footnotesize", 
                            header=FALSE, title="Book-categories", rownames=FALSE,
                            label="categories:6books")
```

Obviously the corpus is very diverse. LDA should not have problems to cluster the chapters of the books.

#### Reduction of the dimensionality

In the set up we have another parameter to adjust. The function *convert_to_dtm* takes the parameter *minfq*, which is used to reduce the "bag of words" (i.e. dimensionality). *minfq* is the minimum frequency for the bag of words dictionary. I will refer to this as "embedding". Let us set it to 2 in this case, meaning that we include a word only if the frequency is 2 or more.

```{r chapters_dtm, chache=TRUE, warning=FALSE}
chapters_dtm <- convert_to_dtm(word_counts, minfq=2)
ncol(chapters_dtm)
```

Let us compare it to the case if including all words.

```{r chapters_dtm_all, chache=TRUE}
chapters_dtm_all <- convert_to_dtm(word_counts, minfq=0)
ncol(chapters_dtm_all)
```

We also want to compare this to a reduction of the word dictionary by the tfidf. For the sake of comparison, the reduction is made to the same value as used above via *minfreq*=2 (i.e. 8597 words).

```{r chapters_dtm_tfidf, chache=TRUE, warning=FALSE}
chapters_dtm_tfidf <- convert_to_dtm_2(word_counts, top=8597)
ncol(chapters_dtm_tfidf)
```


#### Apply the LDA model on the full corpus

##### Fitting via VEM

Set up the LDA model for the shrinked embedding corpus via frequency 2. Using the default "VEM-Algorithm"

```{r chapters_lda, cache=TRUE, warning=FALSE, message=FALSE}
tim1 <- Sys.time()
chapters_lda <- LDA(chapters_dtm, 
                    k = n_books, control = list(seed = 1234))
tim2 <- Sys.time()
u_1 <- tim2-tim1
```

In comparison set up the LDA model for the full word embedding corpus.

```{r chapters_lda_all, cache=TRUE, warning=FALSE, message=FALSE}
tim1 <- Sys.time()
chapters_lda_all <- LDA(chapters_dtm_all, 
                    k = n_books, control = list(seed = 1234))
tim2 <- Sys.time()
u_all <- tim2-tim1
```

Third LDA setup is for the shrinkage of the dictionary by TFIDF.

```{r chapters_lda_tfidf, cache=TRUE, warning=FALSE, message=FALSE}
tim1 <- Sys.time()
chapters_lda_tfidf <- LDA(chapters_dtm_tfidf, 
                    k = n_books, control = list(seed = 1234))
tim2 <- Sys.time()
u_tfidf <- tim2-tim1
chapters_lda
```


Now we evaluate the model all in once: 

```{r, cache=TRUE}
ext_gamma_matrix <- function(model){
  # get gamma matrix for chapter probabilities
  chapters_gamma <- tidy(model, matrix = "gamma")
  # split joint name of book and chapter
  chapters_gamma <- chapters_gamma %>%
    separate(document, c("gutenberg_id", "chapter"), sep = "_", convert = TRUE)
  # get matrix with probabilities for each topic per chapter
  # this matrix is just information and will in this form of the function 
  # not be returned
  gamma_per_chapter <- chapters_gamma %>%
    spread(topic, gamma)
  return(chapters_gamma)
}

validate_LDAclassification <- function(gamma_matrix){
  # gamma_matrix is an object of the function ext_gamma_matrix()
  
  #First we’d find the topic that was most associated with 
  # each chapter using top_n(), which is effectively the 
  # “classification” of that chapter
  chapter_classifications <- gamma_matrix %>%
    group_by(gutenberg_id, chapter) %>%
    top_n(1, gamma) %>%
    ungroup()
  
  # We can then compare each to the “consensus” 
  # topic for each book (the most common topic among its chapters), 
  # and see which were most often misidentified.
  book_topics <- chapter_classifications %>%
    count(gutenberg_id, topic) %>%
    group_by(gutenberg_id) %>%
    # just keep the most frequent one
    top_n(1, n) %>%
    ungroup() %>%
    # keep title called census and topic
    transmute(consensus = gutenberg_id, topic)
  
  # check the fraction of missclassification
  chapter_classifications %>%
    inner_join(book_topics, by = "topic") %>%
    # missmatches
    dplyr::filter(gutenberg_id != consensus)%>%
    nrow()/nrow(chapter_classifications)
}
```

Now we exclude 3 times the beta matrix and evaluate the most likely result with the real results. Depending on how good the LDA will seperate the books, this influences the goodness of the fit.

```{r misc.rate_1, cache=TRUE}
misc.rate_1 <- ext_gamma_matrix(chapters_lda) %>% 
  validate_LDAclassification()
```

```{r misc.rate_all, cache=TRUE}
misc.rate_all <- ext_gamma_matrix(chapters_lda_all) %>% 
  validate_LDAclassification()
```

```{r misc.rate_tfidf, cache=TRUE}
misc.rate_tfidf <- ext_gamma_matrix(chapters_lda_tfidf) %>% 
  validate_LDAclassification()
```

Following matrix gives an overview:

```{r, results="asis"}
performance_matrix <- data.frame(freq2.embedding=c(misc.rate_1, u_1), 
           all.embedding=c(misc.rate_all, u_all),
           tfidf=c(misc.rate_tfidf, u_tfidf))
rownames(performance_matrix) <- c("missc. rate", "time")
performance_matrix %>%  stargazer(summary=FALSE, header=F)
```

Surprisingly, the run-time to fit the LDA model for the embedding using all words, does not take way longer than the embedding using a lower frequency. The fit of the model with the reduced bag of words via tfidf takes slightly less time.

##### Comparison to fitting via Gibbs Sampling

For comparison, we will check the results and the run time for the fit via Gibbs Sampling.

```{r, cache=TRUE, warning=FALSE, message=FALSE}
tim1 <- Sys.time()
chapters_lda_Gibbs <- LDA(chapters_dtm, method="Gibbs",
                    k = n_books, control = list(seed = 1234))
tim2 <- Sys.time()
u_1_Gibbs <- tim2-tim1
```

```{r chapters_lda_all_Gibbs, cache=TRUE, warning=FALSE, message=FALSE}
tim1 <- Sys.time()
chapters_lda_all_Gibbs <- LDA(chapters_dtm_all, method = "Gibbs",
                    k = n_books, control = list(seed = 1234))
tim2 <- Sys.time()
u_all_Gibbs <- tim2-tim1
```

```{r chapters_lda_tfidf_Gibbs, cache=TRUE, warning=FALSE, message=FALSE}
tim1 <- Sys.time()
chapters_lda_tfidf_Gibbs <- LDA(chapters_dtm_tfidf,  method="Gibbs",
                    k = n_books, control = list(seed = 1234))
tim2 <- Sys.time()
u_tfidf_Gibbs <- tim2-tim1
```

```{r misc.rate_1_Gibbs, cache=TRUE}
misc.rate_1_Gibbs <- ext_gamma_matrix(chapters_lda_Gibbs) %>% 
  validate_LDAclassification()
```

```{r misc.rate_all_Gibbs, cache=TRUE}
misc.rate_all_Gibbs <- ext_gamma_matrix(chapters_lda_all_Gibbs) %>% 
  validate_LDAclassification()
```

```{r misc.rate_tfidf_Gibbs, cache=TRUE}
misc.rate_tfidf_Gibbs <- ext_gamma_matrix(chapters_lda_tfidf_Gibbs) %>% 
  validate_LDAclassification()
```

```{r Gibbs_overview, results="asis", cache=TRUE}
performance_matrix <- data.frame(freq2.embedding=c(misc.rate_1_Gibbs, u_1_Gibbs), 
           all.embedding=c(misc.rate_all_Gibbs, u_all_Gibbs),
           tfidf=c(misc.rate_tfidf_Gibbs, u_tfidf_Gibbs))
rownames(performance_matrix) <- c("missc. rate", "time")
performance_matrix %>%  stargazer(summary=FALSE, header=F)
```

Apparently Gibbs Sampling takes a bit longer than the VEM algorithm, but its results with regards to the correct "classification" (missclassification rate) are way better.

## Evaluate model on testing set 

First we split the data randomly in training and testing samples.

```{r split_for_fit_function, cache=TRUE}
split_for_fit <- function(data, test_ratio=0.1, seed=1234){
  # function to set up the training
  # and the testing set from the input data which
  # is an object from the function convert_to_dtm()
  set.seed(seed)
  N <- nrow(data)
  n_test <- (N*test_ratio) %>% ceiling
  test_ind <- sample(1:N, n_test)
  train_ind <- (1:N)[-test_ind]
  ret <- list(train=data[train_ind,],
              test=data[test_ind,])
  return(ret)
}
```

The fit_n_evaluate() function will fit the LDA model and evaluate the goodness of fit for an object of the function split_for_fit. The section Validation gives insights in the procedure of how the "consensus"" is set up.

```{r fitting-function, chache=TRUE}
fit_n_evaluate <- function(split, k=n_books){
  LDA_model <- LDA(split$train, method="Gibbs",
                            k = k, control = list(seed = 1234))
  # use the predict function of udpipe
  # the topic predict funtion already extract the most likely topics
  prediction <- predict(LDA_model, newdata=split$test) %>% .$topic
  # get "consensus" via maximum likelihood
  # first extract the gamma matrix of the model fitted on the training
  # data
  chapters_gamma <- ext_gamma_matrix(LDA_model)
  spreaded_gamma <- chapters_gamma  %>% spread(topic, gamma)
  # get pdfs
  plotm <- spreaded_gamma %>%
    group_by(gutenberg_id) %>%
    # note: pdfs are unnormalized
    summarise_at(2:(titles$len+1), sum)
  topic_link <- plotm %>% 
    apply(1, function(x) which.max(x[2:length(x)])) %>% 
    cbind(plotm$gutenberg_id) %>%
    as.data.frame()
  # exclude the 
  consensus <- split$test %>% 
    rownames() %>% 
    substr(1,regexpr("_",.)-1) %>% 
    as.numeric() %>%
    as.data.frame() %>% 
    # merge it to the topic
    merge(topic_link, by.y="V2", sort=FALSE) %>%
    select(..y)
  # missclassification rate will be returned
  sum(consensus!=prediction)/length(prediction)
}
```

We now can evaluate several fits of a model for different splits. Here is a parallelization for the individual for loops applied.

```{r parallel_fitting_1, cache=TRUE, message=FALSE, warnings=FALSE}
# setting up how many cores to be used
useable_cores <- parallel::detectCores() - 1
# registering cluster
cl <- parallel::makeCluster(useable_cores)
doParallel::registerDoParallel(cl)
n <- 59
results <- foreach(i = 1:n, .combine = 'c', .export = ls(.GlobalEnv), .packages = c("dplyr", "udpipe", "topicmodels", "tidyr", "tidytext")) %dopar% {
  
  
  chapters_dtm %>% 
    split_for_fit(seed=12*i) %>% 
    fit_n_evaluate()
  
  
}
parallel::stopCluster(cl)
```

This is the output of the simulation over 59 splits and fits. The mean is the final result:

```{r}
results %>% summary
```

# Validation

Since the LDA algorithm just clusters into $k$ topics, it is necessary to evaluate which "topic" refers to which book, in order to calculate a missclassification rate. For each of the books we naively derive a distribution of the assignment to the topics. This is done by accumulating the distributions for each chapter of the book. The most likely assignment of the LDA model is choosen as the "correct" topic. Now calculating the missclassification rate is basically the fraction of those chapters/documents not coinciding with the most likely assignment.



```{r, chache=TRUE}
LDA_model <- LDA(chapters_dtm_all, method="Gibbs",
                            k = n_books, control = list(seed = 1234))
# get gamma matrix for chapter probabilities
chapters_gamma <- tidy(LDA_model, matrix = "gamma") %>%
  # split joint name of book and chapter
    separate(document, c("gutenberg_id", "chapter"), sep = "_", convert = TRUE)
spreaded_gamma <- chapters_gamma  %>% spread(topic, gamma)
  # get pdfs
plotm <- spreaded_gamma %>%
    group_by(gutenberg_id) %>%
    # note: pdfs are unnormalized
    summarise_at(2:(titles$len+1), sum)
topic_link <- plotm %>% 
    apply(1, function(x) which.max(x[2:length(x)])) %>% 
    cbind(plotm$gutenberg_id) %>%
    as.data.frame()
  
par(mfrow=c(2,3))
for (i in 1:n_books){
  vec <- plotm[i,2:n_books] %>% unlist() 
  barplot(vec/sum(vec), main=paste("Gutenberg ID", plotm[i,1]))
}

```

This does not exclude the fact that two books are assigned to the same topic (in this case e.g. 2 books are assigned to chapter 1 and none to chapter 6). But still, from each of the plots of the "distributions" you can obtain how good the chapters of a single book are classified. The more of the mass of the distribution is on a single topic, the better the chapters of this topic are predicted.

# Example 2 (10 books)

First we will sample to books again as done in the example 1.

```{r Sampling10books, chache=TRUE, warning=FALSE}
n_books <- 10
by_chapter2 <- set_up_books(n_books=n_books, seed=123456)
appended_by_chapter2 <- append_by_chapter(x=by_chapter2, n_books = n_books)
word_counts2 <- exclude_stop_words(appended_by_chapter2)

```

```{r, results="asis"}
titles <- get_titles(appended_by_chapter2, n_books)
titles$titles %>% stargazer(summary=FALSE, font.size = "footnotesize", 
                            header=FALSE, title="Book-titles Example 2", rownames=FALSE,
                            label="titles:6books")
```

```{r, chache=TRUE}
chapters_dtm_all2 <- convert_to_dtm(word_counts2, minfq=0)
```

## Evaluate model

The same code as for Example 1 is used to evaluate the model. 

```{r parallel_fitting_2, cache=TRUE, message=FALSE, warnings=FALSE}
# registering cluster
cl <- parallel::makeCluster(useable_cores)
doParallel::registerDoParallel(cl)
n <- 59
results <- foreach(i = 1:n, .combine = 'c', .export = ls(.GlobalEnv), .packages = c("dplyr", "udpipe", "topicmodels", "tidyr", "tidytext")) %dopar% {
  
  
  chapters_dtm_all2 %>% 
    split_for_fit(seed=12*i) %>% 
    fit_n_evaluate()
  
  
}
parallel::stopCluster(cl)
```

This is the output of the simulation over 59 splits and fits. The mean is the final result:

```{r}
results %>% summary
```

```{r Plot, cache=TRUE}
LDA_model2 <- LDA(chapters_dtm_all2, method="Gibbs",
                            k = n_books, control = list(seed = 1234))
  # get "consensus" via maximum likelihood
  # first extract the gamma matrix of the model fitted on the training
  # data
chapters_gamma <- ext_gamma_matrix(LDA_model2)
spreaded_gamma <- chapters_gamma  %>% spread(topic, gamma)
  # get pdfs
plotm <- spreaded_gamma %>%
    group_by(gutenberg_id) %>%
    # note: pdfs are unnormalized
    summarise_at(2:(titles$len+1), sum)
topic_link <- plotm %>% 
    apply(1, function(x) which.max(x[2:length(x)])) %>% 
    cbind(plotm$gutenberg_id) %>%
    as.data.frame()
  
par(mfrow=c(2,2))
for (i in 1:n_books){
  vec <- plotm[i,2:n_books] %>% unlist() 
  barplot(vec/sum(vec), main=paste("Gutenberg ID", plotm[i,1]))
}
```

