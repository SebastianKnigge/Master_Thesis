---
title: "Regulatory documents via LDA (adapted documents)"
author: "Sebastian Knigge"
date: "18 8 2019"
output: 
  pdf_document:
    number_sections: true
---

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stargazer)
```

Following libraries are used in the code:

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(tidytext)
library(pdftools)
library(tidyr)
library(stringr)
library(tidytext)
library(udpipe)
library(topicmodels)
library(ggplot2)
library(wordcloud)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(RCurl)
library(XML)
library(openxlsx)
library(keras)
```

# Import data

The Documents had to be preprocessed. For the documents wp2.5 all list of contents had to be deleted, because they were the same in each of these documents. No more adjustments had to be made.

In this code reulatory documents are red in and processed via LDA. This first part focusses on reading in the pdf documents.

```{r, message=FALSE, warning=FALSE, results='asis'}
# getting the right order
setwd('..')
documents <- read.xlsx("Docs_classes.xlsx")[,2]
classes <- read.xlsx("Docs_classes.xlsx")[,c(1,3)]
documents <- paste0(documents,".pdf")
documents %>% as.data.frame() %>% stargazer(summary=FALSE, header = FALSE, title="Document Titles")
```

```{r, message=FALSE, warning=FALSE}
# getting the right directory
library(here)
setwd("../")
path <- getwd() %>% 
  file.path("TextDocs")
setwd(path)
```

Following functions are used to set up and analyze the pdfs.

```{r, message=FALSE, warning=FALSE}
read_pdf_clean <- function(document){
  # This function loads the document given per name
  # and excludes the stop words inclusive numbers
  pdf1 <- pdf_text(file.path(path, document)) %>% 
    strsplit(split = "\n") %>% 
    do.call("c",.) %>% 
    as_tibble() %>%
    unnest_tokens(word,value) %>%
    # also exclude all words including numbers and special characters
    filter(grepl("^[a-z]+$", word))
  # load stopword library
  data(stop_words)
  # add own words to stop word library - here the numbers from 1 to 10
  new_stop_words <- tibble(word=as.character(0:9),
                           lexicon=rep("own",10)) %>%
                           bind_rows(stop_words)
  # stop words are excluded via anti_join
  pdf1 %>% 
    anti_join(new_stop_words)
}

plot_most_freq_words <- function(pdf, n=7){
  # plots a bar plot via ggplot
  pdf %>% count(word) %>% arrange(desc(n)) %>% head(n) %>% 
    ggplot(aes(x=word,y=n)) + 
    geom_bar(stat="identity")+
    # no labels for x and y scale
    theme(axis.title.y=element_blank(),
          axis.title.x=element_blank())
}
```

Now we can read in all documents in a for loop:

```{r, message=FALSE, warning=FALSE}
setwd(path)
# inital set up for the corpus
pdf1 <- read_pdf_clean(documents[1])
corpus <- tibble(document=1, word=pdf1$word)
# adding the documents iteratively
for (i in 2:length(documents)){
  pdf_i <- read_pdf_clean(documents[i])
  corpus <- tibble(document=i, word=pdf_i$word) %>% bind_rows(corpus,.)
}
```

# LDA

The LDA model is applied. First the document term matrix has to be set up. 

```{r, message=FALSE, warning=FALSE}
dtm <- corpus %>% count(document, word, sort = TRUE) %>%
  select(doc_id=document, term=word, freq=n) %>%
  document_term_matrix()
c(N,M) %<-% dim(dtm)
```

Using the function LDA sets up the model and prediction/evaluation is done via predict(). But first of all it shall be verified whether the Predict function actually delivers the same classification as the export of the gamma matrix directly from the LDA model. Therefore both gamma matrices of the single functions are compared. Table \ref{predict} displays the output of the gamma matrix received by the predict() function and Table \ref{extract} displays the gamma matrix returned by the LDA model itself.

```{r LDA, message=FALSE, warning=FALSE, results="asis"}
set.seed(123)
documents_lda <- LDA(dtm, method = "Gibbs",
                    k = 7, control = list(seed = 1234))
```

```{r, message=FALSE, warning=FALSE, results="asis"}
prediction5 <- predict(documents_lda, newdata=dtm, type="topic")

prediction5 <- merge(prediction5, classes, by.x="doc_id", by.y="No")

prediction5 %>% 
  select(doc_id,topic_001,topic_002,topic_003,topic_004,topic_005, topic_006, topic_007) %>% 
  mutate_each(funs(as.numeric), doc_id,topic_001,topic_002,topic_003,topic_004,topic_005, topic_006, topic_007) %>%
  arrange(desc(-doc_id)) %>%
  round(2) %>%
  stargazer(summary=F, rownames = F, header = F, title="Gamma matrix for predict function", label="predict")
```

```{r, message=FALSE, warning=FALSE, results="asis"}
ext_gamma_matrix <- function(model=documents_lda){
  # get gamma matrix for chapter probabilities
  chapters_gamma <- tidy(model, matrix = "gamma")
  # get matrix with probabilities for each topic per chapter
  spreaded_gamma <- chapters_gamma %>% spread(topic, gamma)
  spreaded_gamma %>% 
    mutate_each(funs(as.numeric), document,1,2,3,4,5,6,7) %>%
  arrange(desc(-document))
}

ext_gamma_matrix(documents_lda) %>%
  round(2) %>% 
  stargazer(summary=F, rownames = F, header=F, title="Gamma matrix extracted from model", label="extract")

```

The tables below summarize which document refers to which topic, according to the LDA model.

```{r, message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
library(stargazer)
ind1 <- prediction5 %>% filter(topic==1) %>% select(doc_id, Group) 
ind2 <- prediction5 %>% filter(topic==2) %>% select(doc_id, Group) 
ind3 <- prediction5 %>% filter(topic==3) %>% select(doc_id, Group)
ind4 <- prediction5 %>% filter(topic==4) %>% select(doc_id, Group)
ind5 <- prediction5 %>% filter(topic==5) %>% select(doc_id, Group)
ind6 <- prediction5 %>% filter(topic==6) %>% select(doc_id, Group)
ind7 <- prediction5 %>% filter(topic==7) %>% select(doc_id, Group)
for (i in 1:7){
  cbind(data.frame(Topic=rep(i,nrow(get(paste0("ind",i))))), get(paste0("ind",i))) %>%
  stargazer(summary=FALSE, header = F, rownames=FALSE, title=paste("Documents for Topic", i))
}
```


# Wordclouds

To check what topics tackle which context, we produce wordclouds using the TFIDF and the TF itself.

```{r, message=FALSE, warning=FALSE}
plot_wordcloud <- function(corpus, selection="ALL", max.words=25, i, freq="tfidf"){
  # setting up a tibble which returns tfidf and tf and frequency for 
  # the whole corpus
  tfidf <- corpus %>% count(document, word, sort = TRUE) %>% 
    bind_tf_idf(word, document, n)
  # include all documents for selection if selection="ALL"
  if (all(selection=="ALL")) {
    selection <- corpus %>% 
      select(document) %>% 
      unique() %>% 
      unlist() %>% 
      sort()}
  # filter for all selected documents
  # use either ft or tfidf
  if (freq=="tfidf"){
    dtm_selected <- tfidf %>% filter(document%in%selection) %>% 
      select(word, tf_idf) %>% count(word, wt=tf_idf, sort=TRUE)
  } else {
    dtm_selected <- tfidf %>% filter(document%in%selection) %>% 
      select(word, tf) %>% count(word, wt=tf, sort=TRUE)
  }
  wordcloud(words = dtm_selected$word, freq = dtm_selected$n, min.freq = 1,
            max.words=max.words, random.order=FALSE, 
            colors=brewer.pal(8, "Dark2"), scale=c(3,0.2), 
            main="Title", use.r.layout = TRUE)
  text(x=0.5, y=1, paste("Topic", i))
}
```

For getting specific and more individual words for each cloud, we use the TFIDF in the first step.







## Wordclouds using tfidf

```{r wordcloud_tfidf, message=FALSE, warning=FALSE}
par(mfrow=c(2,4))
par(mar=c(1,1,0.5,1))
plot_wordcloud(corpus, selection=ind1[,1], i=1) %>% unlist() %>% as.integer()
plot_wordcloud(corpus, selection=ind2[,1], i=2) %>% unlist() %>% as.integer()
plot_wordcloud(corpus, selection=ind3[,1], i=3) %>% unlist() %>% as.integer()
plot_wordcloud(corpus, selection=ind4[,1], i=4) %>% unlist() %>% as.integer()
plot_wordcloud(corpus, selection=ind5[,1], i=5) %>% unlist() %>% as.integer()
plot_wordcloud(corpus, selection=ind5[,1], i=6) %>% unlist() %>% as.integer()
plot_wordcloud(corpus, selection=ind5[,1], i=7) %>% unlist() %>% as.integer()  
```

## Wordclouds using tf

The same can be done using the regular term frequency.

```{r wordcloud_tf, message=FALSE, warning=FALSE, cache=TRUE}
par(mfrow=c(2,4))
par(mar=c(1,1,0.5,1))
plot_wordcloud(corpus, selection=ind1[,1], i=1, freq="tf")
plot_wordcloud(corpus, selection=ind2[,1], i=2, freq="tf")
plot_wordcloud(corpus, selection=ind3[,1], i=3, freq="tf")
plot_wordcloud(corpus, selection=ind4[,1], i=4, freq="tf")
plot_wordcloud(corpus, selection=ind5[,1], i=5, freq="tf") 
plot_wordcloud(corpus, selection=ind5[,1], i=6, freq="tf") 
plot_wordcloud(corpus, selection=ind5[,1], i=7, freq="tf")  
```

# Embedding via tfidf

Now it's interesting to see if embedding with tfidf will cluster other groups or the same. So we will reduce the Document Term Matrix to M*0.8 words which is a reduction by approx. 20\%.

```{r}
dtm_50 <- dtm %>% dtm_remove_tfidf(top=M*0.8)
set.seed(123)
documents_lda_2 <- LDA(dtm_50, method="Gibbs",
                    k = 7, control = list(seed = 1234))
```

```{r}
prediction5_2 <- predict(documents_lda_2, newdata=dtm_50, type="topic")
prediction5_2 <- merge(prediction5_2, classes, by.x="doc_id", by.y="No")
# compare topic 1 with topic 2, 3, 4 and 5
ind1_2 <- prediction5_2 %>% filter(topic==1) %>% select(doc_id, Group)
ind2_2 <- prediction5_2 %>% filter(topic==2) %>% select(doc_id, Group)
ind3_2 <- prediction5_2 %>% filter(topic==3) %>% select(doc_id, Group)
ind4_2 <- prediction5_2 %>% filter(topic==4) %>% select(doc_id, Group)
ind5_2 <- prediction5_2 %>% filter(topic==5) %>% select(doc_id, Group)
ind6_2 <- prediction5_2 %>% filter(topic==6) %>% select(doc_id, Group)
ind7_2 <- prediction5_2 %>% filter(topic==7) %>% select(doc_id, Group)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
for (i in 1:7){
  cbind(data.frame(Topic_embedding_0.8=rep(i,nrow(get(paste0("ind",i,"_2"))))), get(paste0("ind",i,"_2"))) %>%
  stargazer(summary=FALSE, header = F, rownames=FALSE, title=paste("Documents for Topic", i))
}
```

```{r, message=FALSE, warning=FALSE, chache=TRUE, results="asis"}
ext_gamma_matrix(documents_lda_2) %>%
  round(2) %>% 
  stargazer(summary=F, rownames = F, header=F, title="Gamma matrix extracted from model for embedding with tfidf", label="extract2")
```

## Wordclouds

```{r wordcloud_tfidf2, message=FALSE, warning=FALSE}
par(mfrow=c(2,4))
par(mar=c(1,1,0.5,1))
plot_wordcloud(corpus, selection=ind1_2[,1], i=1)
plot_wordcloud(corpus, selection=ind2_2[,1], i=2)
plot_wordcloud(corpus, selection=ind3_2[,1], i=3)
plot_wordcloud(corpus, selection=ind4_2[,1], i=4)
plot_wordcloud(corpus, selection=ind5_2[,1], i=5) 
plot_wordcloud(corpus, selection=ind4_2[,1], i=6)
plot_wordcloud(corpus, selection=ind5_2[,1], i=7) 
```

We want to give an overview over the clustered documents with the respective wordclouds.

```{r, message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
for (i in 1:7){
  cbind(data.frame(Topic=rep(i,nrow(get(paste0("ind",i,"_2"))))), get(paste0("ind",i,"_2"))) %>%
  stargazer(summary=FALSE, header = F, rownames=FALSE, title=paste("Documents for Topic", i))
  plot_wordcloud(corpus, selection=get(paste0("ind",i,"_2"))[,1], i=i)
}
```

Now we use the validation measure we used for the Example 1. 

```{r}
validate_LDAclassification <- function(predict_table){
  # gamma_matrix is an object of the function ext_gamma_matrix()
  
  # First weâ€™d find the topic that was most associated with 
  # each chapter 
  conversion <- predict_table %>%
    select(Group, topic) %>% 
    group_by(Group) %>%
    top_n(1,topic) %>%
    unique()
    
  predict_table %>% 
    left_join(conversion, by=c("topic")) %>%
    filter(Group.x!=Group.y) %>%
    nrow()/nrow(predict_table)
}
```

On both full bag of words and 80\% embedding via Tfidf
```{r}
predict_table <- prediction5 %>% select(doc_id, topic) %>% 
  merge( y=classes, by.x=1, by.y=1)

validate_LDAclassification(predict_table)

predict_table2 <- prediction5_2 %>% select(doc_id, topic) %>% 
  merge( y=classes, by.x=1, by.y=1)

validate_LDAclassification(predict_table2)
```

