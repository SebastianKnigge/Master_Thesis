---
title: "Regulatory Documents via LDA - Documentation"
author: "Sebastian Knigge"
date: "18 8 2019"
output: 
  pdf_document:
    number_sections: true
---
\pagenumbering{gobble}
\tableofcontents

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stargazer)
```

Following libraries are used in the code:

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(tidytext)
library(pdftools)
library(tidyr)
library(stringr)
library(tidytext)
library(udpipe)
library(topicmodels)
library(ggplot2)
library(wordcloud)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(RCurl)
library(XML)
library(openxlsx)
library(keras)
library(foreach)
library(doParallel)
```

# Import data

The Documents had to be preprocessed. For the documents wp2.5 all list of contents had to be deleted, because they were the same in each of these documents. No more adjustments had to be made.

In this code reulatory documents are red in and processed via LDA. This first part focusses on reading in the pdf documents.

```{r, message=FALSE, warning=FALSE, results='asis'}
# getting the right order
setwd('..')
documents <- read.xlsx("Docs_classes.xlsx")[,2]
classes <- read.xlsx("Docs_classes.xlsx")[,c(1,3)]
documents <- paste0(documents,".pdf")
documents %>% as.data.frame() %>% stargazer(summary=FALSE, 
                                            header = FALSE, 
                                            title="Document Titles")
```

```{r, message=FALSE, warning=FALSE}
# getting the right directory
library(here)
setwd("../")
path <- getwd() %>% 
  file.path("TextDocs")
setwd(path)
```

Following functions are used to set up and analyze the pdfs. When cleaning up data, we have to take into account certain circumstances of the regulatory documents. For example, there are many formulas and technical abbreviations in the documents. Every variable, every estimator, and every index is included as a single word in the bag of words. These terms sometimes have a big influence on the documents, because they are very specific for individual documents and occur quite often. To avoid this, we exclude all mixed words with characters and numeric values, as well as all terms with special characters (e.g. Greek letters).

```{r, message=FALSE, warning=FALSE}
read_pdf_clean <- function(document){
  # This function loads the document given per name
  # and excludes the stop words 
  pdf1 <- pdf_text(file.path(path, document)) %>% 
    strsplit(split = "\n") %>% 
    do.call("c",.) %>% 
    as_tibble() %>%
    unnest_tokens(word,value) %>%
    # also exclude all words which include numbers and special characters
    filter(grepl("^[a-z]+$", word))
  # load stopword library
  data(stop_words)
  # stop words are excluded via anti_join
  pdf1 %>% 
    anti_join(stop_words)
}

plot_most_freq_words <- function(pdf, n=7){
  # plots a bar plot via ggplot
  pdf %>% count(word) %>% arrange(desc(n)) %>% head(n) %>% 
    ggplot(aes(x=word,y=n)) + 
    geom_bar(stat="identity")+
    # no labels for x and y scale
    theme(axis.title.y=element_blank(),
          axis.title.x=element_blank())
}
```

Now we can read in all documents using a for loop:

```{r, message=FALSE, warning=FALSE}
setwd(path)
# inital set up for the corpus
pdf1 <- read_pdf_clean(documents[1])
corpus <- tibble(document=1, word=pdf1$word)
# adding the documents iteratively
for (i in 2:length(documents)){
  pdf_i <- read_pdf_clean(documents[i])
  corpus <- tibble(document=i, word=pdf_i$word) %>% bind_rows(corpus,.)
}
```

# LDA

The LDA model is applied. First the document term matrix has to be set up. 

```{r, message=FALSE, warning=FALSE}
dtm <- corpus %>% count(document, word, sort = TRUE) %>%
  select(doc_id=document, term=word, freq=n) %>%
  document_term_matrix()
# dimensions
c(N,M) %<-% dim(dtm)
N; M
```

We use term frequency 2 embedding because in the example with the Gutenberg Data, it turned out to be advantageous with regard to the "predictive power" of the LDA algorithm.

```{r}
dtm_tf2 <- dtm %>%
    # reduce by low frequencies
    dtm_remove_lowfreq(minfreq = 2)
ncol(dtm_tf2 )
```

Using the function LDA sets up the model and prediction/evaluation is done via $predict()$. But first of all it shall be verified whether the Predict function actually delivers the same classification as the export of the gamma matrix directly from the LDA model. Therefore both gamma matrices of the single functions are compared. Table \ref{predict} displays the output of the gamma matrix received by the $predict()$ function and Table \ref{extract} displays the gamma matrix returned by the LDA model itself.

```{r LDA., message=FALSE, warning=FALSE, results="asis"}
tim1 <- Sys.time()
set.seed(123)
documents_lda <- LDA(dtm_tf2, method = "Gibbs",
                    k = 7, control = list(seed = 1234))
tim2 <- Sys.time()
u1 <- tim2 - tim1
```

```{r, message=FALSE, warning=FALSE, results="asis"}
prediction5 <- predict(documents_lda, newdata=dtm_tf2, type="topic")

prediction5 <- merge(prediction5, classes, by.x="doc_id", by.y="No")

prediction5 %>% 
  select(doc_id,topic_001,topic_002,topic_003,topic_004,topic_005, 
         topic_006, topic_007) %>% 
  mutate_each(funs(as.numeric), 
              doc_id,topic_001,topic_002,topic_003,topic_004,
              topic_005, topic_006, topic_007) %>%
  arrange(desc(-doc_id)) %>%
  round(2) %>%
  stargazer(summary=F, rownames = F, header = F, 
            title="Gamma matrix for predict function", label="predict")
```

```{r, message=FALSE, warning=FALSE, results="asis"}
ext_gamma_matrix <- function(model=documents_lda){
  # get gamma matrix for chapter probabilities
  chapters_gamma <- tidy(model, matrix = "gamma")
  # get matrix with probabilities for each topic per chapter
  spreaded_gamma <- chapters_gamma %>% spread(topic, gamma)
  spreaded_gamma %>% 
    mutate_each(funs(as.numeric), document,1,2,3,4,5,6,7) %>%
  arrange(desc(-document))
}

ext_gamma_matrix(documents_lda) %>%
  round(2) %>% 
  stargazer(summary=F, rownames = F, header=F, 
            title="Gamma matrix extracted from model", 
            label="extract")

```

The tables below summarize which document refers to which topic, according to the LDA model (term frequency 2 embedding).

```{r, message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
library(stargazer)
ind1 <- prediction5 %>% filter(topic==1) %>% select(doc_id, Group) 
ind2 <- prediction5 %>% filter(topic==2) %>% select(doc_id, Group) 
ind3 <- prediction5 %>% filter(topic==3) %>% select(doc_id, Group)
ind4 <- prediction5 %>% filter(topic==4) %>% select(doc_id, Group)
ind5 <- prediction5 %>% filter(topic==5) %>% select(doc_id, Group)
ind6 <- prediction5 %>% filter(topic==6) %>% select(doc_id, Group)
ind7 <- prediction5 %>% filter(topic==7) %>% select(doc_id, Group)
for (i in 1:7){
  cbind(data.frame(Topic=rep(i,nrow(get(paste0("ind",i))))), 
        get(paste0("ind",i))) %>%
  stargazer(summary=FALSE, header = F, rownames=FALSE, 
            title=paste("Documents for Topic", i))
}
```


## Wordclouds

To check what topics tackle which context, we produce wordclouds using the *tf-idf* and the *tf* itself.

```{r, message=FALSE, warning=FALSE}
plot_wordcloud <- function(corpus, selection="ALL", 
                           max.words=50, i, freq="tfidf", 
                           scale=c(3,0.2)){
  # setting up a tibble which returns tfidf and tf and frequency for 
  # the whole corpus
  tfidf <- corpus %>% count(document, word, sort = TRUE) %>% 
    bind_tf_idf(word, document, n)
  # include all documents for selection if selection="ALL"
  if (all(selection=="ALL")) {
    selection <- corpus %>% 
      select(document) %>% 
      unique() %>% 
      unlist() %>% 
      sort()}
  # filter for all selected documents
  # use either ft or tfidf
  if (freq=="tfidf"){
    dtm_selected <- tfidf %>% filter(document%in%selection) %>% 
      select(word, tf_idf) %>% count(word, wt=tf_idf, sort=TRUE)
  } else {
    dtm_selected <- tfidf %>% filter(document%in%selection) %>% 
      select(word, tf) %>% count(word, wt=tf, sort=TRUE)
  }
  # plotting
  wordcloud(words = dtm_selected$word, freq = dtm_selected$n, 
            min.freq = 1,max.words=max.words, random.order=FALSE, 
            colors=brewer.pal(8, "Dark2"), scale=scale, 
            main="Title", use.r.layout = TRUE)
  # set a title = document
  text(x=0.5, y=1, paste("Topic", i))
}
```

A second possibility is to extract the *tf-idf*s of the words linked to the topics directly. First you have to map the topics to the documents within the tidytext format. This is the only way the tfidf_tf matrix can be set up for the individual topics.

```{r, message=FALSE, warning=FALSE}
plot_wordcloud_topic <- function(corpus, topic_select=1, prediction=prediction5,
                           max.words=50, 
                           scale=c(3,0.2)){
  # get the correct topic mapping via the first two columns 
  # of the prediction matrix
  new_corpus <- prediction %>% 
    transmute(doc_id=as.numeric(doc_id), topic) %>%
    right_join(corpus, c("doc_id"="document"))
  # setting up a tibble which returns tfidf and tf and frequency for 
  # the whole corpus
  tfidf <- new_corpus %>% count(topic, word, sort = TRUE) %>% 
    bind_tf_idf(word, topic, n) %>%
    # filter for all selected topics
    # use either ft or tfidf
    filter(topic==topic_select)
  # plotting
  wordcloud(words = tfidf$word, freq = tfidf$tf_idf, 
            min.freq = 1,max.words=max.words, random.order=FALSE, 
            colors=brewer.pal(8, "Dark2"), scale=scale, 
            main="Title", use.r.layout = TRUE)
  # set a title = document
  text(x=0.5, y=1, paste("Topic", topic_select))
}
```





### Wordclouds using *tf-idf*

For getting specific and more individual words for each cloud, we use the *tf-idf* in the first step.
Now there are two methods to create the word clouds for the *tf-idf* measure. Once by aggregating the individual *tf-idf*s of the documents, as it is done in the following. 

```{r wordcloud_tfidf, message=FALSE, warning=FALSE}
par(mfrow=c(2,4))
par(mar=c(1,1,1,1))
set.seed(123)
plot_wordcloud(corpus, selection=ind1[,1], i=1, scale=c(1.7,0.001), 
               max.words = 45)
plot_wordcloud(corpus, selection=ind2[,1], i=2)
plot_wordcloud(corpus, selection=ind3[,1], i=3, scale=c(1.6,0.005), 
               max.words = 45)
plot_wordcloud(corpus, selection=ind4[,1], i=4) 
plot_wordcloud(corpus, selection=ind5[,1], i=5, scale=c(1.8,0.001), 
               max.words = 40)
plot_wordcloud(corpus, selection=ind6[,1], i=6, scale=c(1.8,0.001), 
               max.words = 40)
plot_wordcloud(corpus, selection=ind7[,1], i=7, scale=c(2.1,0.3), 
               max.words = 45) 
```

Now using the second approach, when applying the *tf-idf* measure to the mapped corpus.

```{r, message=FALSE, warning=FALSE}
par(mfrow=c(2,4))
par(mar=c(1,1,1,1))
set.seed(123)
plot_wordcloud_topic(corpus, topic_select=1, scale=c(1.9,0.0006), 
               max.words = 40)
plot_wordcloud_topic(corpus, topic_select=2)
plot_wordcloud_topic(corpus, topic_select=3, scale=c(1.6,0.005), 
               max.words = 40)
plot_wordcloud_topic(corpus, topic_select=4, max.words = 50, scale=c(2.3, 0.2))
plot_wordcloud_topic(corpus, topic_select=5, scale=c(1.8,0.01), 
               max.words = 40)
plot_wordcloud_topic(corpus, topic_select=6, scale=c(2,0.1), 
               max.words = 45)
plot_wordcloud_topic(corpus, topic_select=7, scale=c(2.5,0.5), 
               max.words = 50) 
```

Although the second procedure, using *tf-idf* distributions for the individual topics, seems to be more intuitive, the two figures are surprisingly similar. 

### Wordclouds using *tf*

The same can be done using the regular term frequency.

```{r wordcloud_tf, message=FALSE, warning=FALSE, cache=FALSE}
par(mfrow=c(2,4))
par(mar=c(1,1,0.5,1))
set.seed(123)
plot_wordcloud(corpus, selection=ind1[,1], i=1, freq="tf", scale=c(2,0.05))
plot_wordcloud(corpus, selection=ind2[,1], i=2, freq="tf", scale=c(2,0.05))
plot_wordcloud(corpus, selection=ind3[,1], i=3, freq="tf", scale=c(2,0.03), 
               max.words = 40)
plot_wordcloud(corpus, selection=ind4[,1], i=4, freq="tf", scale=c(2,0.3))
plot_wordcloud(corpus, selection=ind5[,1], i=5, freq="tf", scale=c(2.5,0.1)) 
plot_wordcloud(corpus, selection=ind6[,1], i=6, freq="tf") 
plot_wordcloud(corpus, selection=ind7[,1], i=7, freq="tf", scale=c(2.5,0.2), 
               max.words = 45)  
```

## Embedding via *tf-idf*

Now it is interesting to see if embedding via *tf-idf* will cluster other groups or the same. So we will reduce the Document Term Matrix to $5717$ words, which amounts to a reduction by 50\%.

```{r, cache=FALSE}
tim1_tfidf <- Sys.time()
dtm_50 <- dtm %>% dtm_remove_tfidf(top=(0.5*M))
set.seed(123)
documents_lda_2 <- LDA(dtm_50, method="Gibbs",
                    k = 7, control = list(seed = 1234))
tim2_tfidf <- Sys.time()
u2 <- tim2_tfidf - tim1_tfidf
```

```{r, cache=FALSE}
prediction5_2 <- predict(documents_lda_2, newdata=dtm_50, type="topic")
prediction5_2 <- merge(prediction5_2, classes, by.x="doc_id", by.y="No")
# compare topic 1 with topic 2, 3, 4 and 5
ind1_2 <- prediction5_2 %>% filter(topic==1) %>% select(doc_id, Group)
ind2_2 <- prediction5_2 %>% filter(topic==2) %>% select(doc_id, Group)
ind3_2 <- prediction5_2 %>% filter(topic==3) %>% select(doc_id, Group)
ind4_2 <- prediction5_2 %>% filter(topic==4) %>% select(doc_id, Group)
ind5_2 <- prediction5_2 %>% filter(topic==5) %>% select(doc_id, Group)
ind6_2 <- prediction5_2 %>% filter(topic==6) %>% select(doc_id, Group)
ind7_2 <- prediction5_2 %>% filter(topic==7) %>% select(doc_id, Group)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
for (i in 1:7){
  cbind(data.frame(Topic_embedding_0.8=rep(i,nrow(get(paste0("ind",i,"_2"))))), 
        get(paste0("ind",i,"_2"))) %>%
  stargazer(summary=FALSE, header = F, rownames=FALSE, 
            title=paste("Documents for Topic", i))
}
```

```{r, message=FALSE, warning=FALSE, chache=TRUE, results="asis"}
ext_gamma_matrix(documents_lda_2) %>%
  round(2) %>% 
  stargazer(summary=F, rownames = F, header=F, 
            title="Gamma matrix extracted from model for embedding with tfidf", 
            label="extract2")
```

We want to give an overview over the clustered documents using the *tf-idf* embedding.

```{r, message=FALSE, warning=FALSE, echo=FALSE, results='asis'}
for (i in 1:7){
  cbind(data.frame(Topic=rep(i,nrow(get(paste0("ind",i,"_2"))))), 
        get(paste0("ind",i,"_2"))) %>%
  stargazer(summary=FALSE, header = F, rownames=FALSE, 
            title=paste("Documents for Topic", i))
}
```

We want to produce wordclouds again.
This time using the *tf-idf* embedding for clustering via LDA. The first plot shows the aggregated *tf-idf*s.

```{r wordcloud_tfidf2, message=FALSE, warning=FALSE}
par(mfrow=c(2,4))
par(mar=c(1,1,0.5,1))
set.seed(123)
plot_wordcloud(corpus, selection=ind1_2[,1], i=1, scale=c(1.7,0.002),
               max.words = 35)
plot_wordcloud(corpus, selection=ind2_2[,1], i=2, scale=c(2.5,0.1))
plot_wordcloud(corpus, selection=ind3_2[,1], i=3, scale=c(1.5,0.001), 
               max.words = 30)
plot_wordcloud(corpus, selection=ind4_2[,1], i=4, scale=c(1.9,0.05), 
               max.words = 40)
plot_wordcloud(corpus, selection=ind5_2[,1], i=5, scale=c(2,0.02), 
               max.words = 35) 
plot_wordcloud(corpus, selection=ind6_2[,1], i=6, scale=c(2,0.1), 
               max.words = 40)
plot_wordcloud(corpus, selection=ind7_2[,1], i=7, scale=c(2,0.03), 
               max.words = 35) 
```

The second plot shows the *tf-idf*s for each individual topic.

```{r, message=FALSE, warning=FALSE}
par(mfrow=c(2,4))
par(mar=c(1,1,1,1))
set.seed(123)
plot_wordcloud_topic(corpus, topic_select=1, scale=c(1.9,0.0006), 
               max.words = 40, prediction=prediction5_2)
plot_wordcloud_topic(corpus, topic_select=2, prediction=prediction5_2, 
                     scale=c(1.9,0.0006))
plot_wordcloud_topic(corpus, topic_select=3, scale=c(1.6,0.005), 
               max.words = 40, prediction=prediction5_2)
plot_wordcloud_topic(corpus, topic_select=4, max.words = 40, 
                     scale=c(2.3, 0.02), prediction=prediction5_2)
plot_wordcloud_topic(corpus, topic_select=5, scale=c(1.8,0.01), 
               max.words = 40, prediction=prediction5_2)
plot_wordcloud_topic(corpus, topic_select=6, scale=c(2,0.1), 
               max.words = 45, prediction=prediction5_2)
plot_wordcloud_topic(corpus, topic_select=7, scale=c(2.5,0.5), 
               max.words = 50, prediction=prediction5_2)
```



## Misclassification Rates

Now we use the validation measure we used for Example 1. 

```{r}
validate_LDAclassification <- function(predict_table){
  # gamma_matrix ... an object of the function ext_gamma_matrix()
  # First weâ€™d find the topic that was most associated with 
  # each chapter 
  conversion <- predict_table %>%
    select(Group, topic) %>% 
    group_by(Group) %>%
    top_n(1,topic) %>%
    unique()
    
  consensus <- predict_table %>% 
    left_join(conversion, by=c("topic"))
  consensus %>% filter(Group.x!=Group.y) %>%
    nrow()/nrow(consensus)
}
```

On both full bag of words and 50\% embedding via *tf-idf*
```{r, cache=FALSE}
predict_table <- prediction5 %>% select(doc_id, topic) %>% 
  merge( y=classes, by.x=1, by.y=1)

( misc.rate_embedding2 <- validate_LDAclassification(predict_table) )
```

```{r, cache=FALSE}
predict_table2 <- prediction5_2 %>% select(doc_id, topic) %>% 
  merge( y=classes, by.x=1, by.y=1)

( misc.rate_embedding_tfidf <- validate_LDAclassification(predict_table2))
```

```{r, results="asis"}
performance_matrix <- data.frame(freq2.embedding=c(misc.rate_embedding2, u1), 
           tfidf.embedding=c(misc.rate_embedding_tfidf, u2))
rownames(performance_matrix) <- c("misc. rate", "time")
performance_matrix %>%  stargazer(summary=FALSE, header=F, title = "LDA via Gibbs Sampling")
```

# Optimal value of Reduction via tfidf

In this example we want to know if there is a better value for the reduction of tfidf than 50\%. We will examine it using the same approach as for the Gutenberg examples. 

```{r}
evaluation_for_embedding <- function(word_counts, frac=0.1, M=8254) {
  # embedding 3
  chapters_dtm_tfidf <- dtm %>% dtm_remove_tfidf(top=(frac*M))
  # fitting
  tim1 <- Sys.time()
  chapters_lda_tfidf_Gibbs <- LDA(chapters_dtm_tfidf,  method="Gibbs",
                    k = 7, control = list(seed = 1234))
  tim2 <- Sys.time()
  u_tfidf_Gibbs <- tim2-tim1
  prediction5_2 <- predict(chapters_lda_tfidf_Gibbs, 
                           newdata=chapters_dtm_tfidf, type="topic")
  prediction5_2 <- merge(prediction5_2, classes, by.x="doc_id", by.y="No")
  # calculating missclassification rate
  predict_table <- prediction5_2 %>% select(doc_id, topic) %>% 
  merge( y=classes, by.x=1, by.y=1)

  misc.rate_tfidf_Gibbs <-validate_LDAclassification(predict_table)
   
  # function returns a vector including the misc. ratio and the fitting time
  return(c(misc.rate_tfidf_Gibbs, 
              as.numeric(u_tfidf_Gibbs)))
}
```


```{r, cache=TRUE}
# set up the fractions we want to use
# we use 0.1, 0.2, ..., 1
fractions <- seq(0.1,1,0.01)

# Use parallelization
# setting up how many cores to be used
useable_cores <- parallel::detectCores() - 1
# registering cluster
cl <- parallel::makeCluster(useable_cores)
doParallel::registerDoParallel(cl)
embedding_performance_matrix <- foreach(i = 1:length(fractions), .combine = 'rbind', .export = ls(.GlobalEnv), .packages = c("dplyr", "udpipe", "topicmodels", "tidyr", "tidytext")) %dopar% {
  
  
  evaluation_for_embedding(dtm, frac=fractions[i]) %>% 
    c(.,fractions[i])
  
  
}
parallel::stopCluster(cl)

# adjust the resulting matrix to a data frame for plotting
colnames(embedding_performance_matrix) <- c("misc.rate","time","fractions")
embedding_performance_matrix <- as.data.frame(embedding_performance_matrix)
```

```{r}
ggplot(data=embedding_performance_matrix, 
       aes(x=fractions, y=misc.rate)) +
  geom_line(col="grey") +
  geom_point() +
  geom_smooth(col="green3", method="loess", span=0.7) +
  theme_minimal() +
  ggtitle("Misclassification rates for different types of tfidf-embeddings") +
  xlab("fraction")
```




We can find an optimal value for *fraction* at the value aroung $0.35$, so it might be interesting to campare the results of a fit using this optimized value of fraction to the other embeddings.

```{r, cache=FALSE}
tim1_tfidf0.85 <- Sys.time()
dtm_0.85 <- dtm %>% dtm_remove_tfidf(top=(0.35*M))
set.seed(123)
documents_lda_0.85 <- LDA(dtm_0.85, method="Gibbs",
                    k = 7, control = list(seed = 1234))
tim2_tfidf0.85 <- Sys.time()
u0.85 <- tim2_tfidf0.85 - tim1_tfidf0.85
```

```{r, cache=FALSE}
prediction0.85 <- predict(documents_lda_0.85, newdata=dtm_0.85, type="topic")
prediction0.85 <- merge(prediction0.85, classes, by.x="doc_id", by.y="No")
```

```{r, cache=FALSE}
predict_table0.85 <- prediction0.85 %>% select(doc_id, topic) %>% 
  merge( y=classes, by.x=1, by.y=1)

( misc.rate_embedding_0.85 <- validate_LDAclassification(predict_table0.85))
```

In Table \ref{comp0.85} you will find an overview of the performace of the different embedding methods including *tfidf* embedding of fraction $0.35$.
```{r, results="asis"}
performance_matrix <- data.frame(freq2.embedding=c(misc.rate_embedding2, u1), 
           tfidf_0.5.embedding=c(misc.rate_embedding_tfidf, u2),
           tfidf_0.85.embedding=c(misc.rate_embedding_0.85, u0.85))
rownames(performance_matrix) <- c("misc. rate", "time")
performance_matrix %>%  stargazer(summary=FALSE, header=F, 
                                  title = "LDA via Gibbs Sampling", label="comp0.85")
```

# Coherence Cloud

```{r, warnings=FALSE, message=FALSE, results='hide'}
dtm_50 %>% as.matrix() %>% t() %>% comparison.cloud(scale=c(2,.05), 
                                                    max.words=50, title.size = 1)
```
